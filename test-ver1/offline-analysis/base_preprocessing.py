import os
import sys
import pandas as pd
import numpy as np
import json
import pickle
from pathlib import Path

class BasePreprocessor:
    """í†µí•© ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.checkpoint_file = "processing_checkpoint.json"
        self.batch_size = 10000  # ë°°ì¹˜ í¬ê¸°
    
    def _fix_year_vectorized(self, s: pd.Series) -> pd.Series:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ë³´ì • - 2ìë¦¬ ì—°ë„ë¥¼ 4ìë¦¬ë¡œ ë³€í™˜"""
        s = s.astype('string').str.strip()
        mask_two = s.str.match(r'^\d{2}[-/.]')
        s = s.mask(mask_two, s.str.replace(r'^(\d{2})([-/.])', r'20\1\2', regex=True))
        return pd.to_datetime(s, errors='coerce')
    
    
    def clean_data(self, df: pd.DataFrame, category: str) -> pd.DataFrame:
        """ë°ì´í„° ì •ì œ"""
        if df.empty:
            return df
            
        df = df.copy()
        
        # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ ê²½ìš° ì œê±°
        if len(df) > 0 and df.iloc[0].astype(str).str.contains('-').any():
            df = df.drop(df.index[0]).reset_index(drop=True)
        
        # ë¹ˆ í–‰ ì œê±° ë° ì»¬ëŸ¼ëª… ì •ì œ
        df = df.dropna(how='all').reset_index(drop=True)
        df.columns = [col.strip() for col in df.columns]
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì •ì œ
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).str.strip()
        
        df = df.replace('', pd.NA)
        
        return df
    
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'r') as f:
                return json.load(f)
        return {}
    
    def save_checkpoint(self, checkpoint_data):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f, indent=2)
    
    def process_file_with_checkpoint(self, file_path: str, category: str) -> pd.DataFrame:
        """ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” íŒŒì¼ ì²˜ë¦¬"""
        checkpoint = self.load_checkpoint()
        file_key = f"{category}_{os.path.basename(file_path)}"
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        total_rows = sum(1 for _ in open(file_path)) - 1  # í—¤ë” ì œì™¸
        
        if file_key in checkpoint:
            processed_rows = checkpoint[file_key].get('processed_rows', 0)
            if processed_rows >= total_rows:
                print(f"â­ï¸ {file_path} ì´ë¯¸ ì™„ë£Œë¨, ê±´ë„ˆë›°ê¸°")
                return pd.DataFrame()
            print(f"ğŸ”„ {file_path} {processed_rows}í–‰ë¶€í„° ì´ì–´ì„œ ì²˜ë¦¬")
        else:
            processed_rows = 0
            print(f"ğŸ†• {file_path} ìƒˆë¡œ ì‹œì‘")
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        all_dfs = []
        batch_start = processed_rows
        
        try:
            while batch_start < total_rows:
                batch_end = min(batch_start + self.batch_size, total_rows)
                
                # ë°°ì¹˜ ì½ê¸°
                df_batch = pd.read_csv(file_path, skiprows=batch_start + 1, nrows=self.batch_size, low_memory=False)
                
                if df_batch.empty:
                    break
                
                # ì „ì²˜ë¦¬
                df_batch = self.clean_data(df_batch, category)
                if category == 'gps':
                    df_batch = self.expand_gps_list_columns(df_batch)
                df_batch = self.validate_physical_ranges(df_batch, category)
                df_batch = self.convert_data_types(df_batch, category)
                df_batch = self.remove_duplicates(df_batch)
                
                all_dfs.append(df_batch)
                
                # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
                checkpoint[file_key] = {
                    'processed_rows': batch_end,
                    'total_rows': total_rows,
                    'status': 'in_progress' if batch_end < total_rows else 'completed'
                }
                self.save_checkpoint(checkpoint)
                
                print(f"âœ… {file_path} {batch_end}/{total_rows} í–‰ ì²˜ë¦¬ ì™„ë£Œ")
                batch_start = batch_end
            
            # ëª¨ë“  ë°°ì¹˜ í•©ì¹˜ê¸°
            if all_dfs:
                result_df = pd.concat(all_dfs, ignore_index=True)
                return result_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ {file_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
    
    def expand_gps_list_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """GPS ë¦¬ìŠ¤íŠ¸í˜• ì»¬ëŸ¼ ë™ì  í™•ì¥ (NaN íŒ¨ë”©)"""
        if df.empty:
            return df
        df = df.copy()
        candidate_delims = [',', '|', ';']
        object_cols = [c for c in df.columns if df[c].dtype == 'object']
        cols_to_expand = []
        for col in object_cols:
            sample = df[col].dropna().astype(str).head(50)
            if sample.empty:
                continue
            for delim in candidate_delims:
                if sample.str.contains(delim).mean() > 0.5:
                    cols_to_expand.append((col, delim))
                    break
        for col, delim in cols_to_expand:
            parts_series = df[col].astype(str).where(df[col].notna(), None)
            max_len = parts_series.dropna().apply(lambda x: len(x.split(delim)) if isinstance(x, str) else 0).max()
            if not max_len or max_len < 2:
                continue
            new_cols = [f"{col}_{i+1}" for i in range(max_len)]
            for i in range(max_len):
                df[new_cols[i]] = parts_series.apply(
                    lambda x: (x.split(delim)[i].strip() if isinstance(x, str) and len(x.split(delim)) > i else np.nan)
                )
            df = df.drop(columns=[col])
        return df

    def validate_physical_ranges(self, df: pd.DataFrame, category: str) -> pd.DataFrame:
        """ë¬¼ë¦¬ì  ë²”ìœ„ ê²€ì¦"""
        if df.empty:
            return df
            
        df = df.copy()
        
        if category == 'bms':
            # SOC, SOH: 0~100%
            soc_soh_cols = [col for col in df.columns if 'soc' in col.lower() or 'soh' in col.lower()]
            for col in soc_soh_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and (val < 0 or val > 100)) else val)
            
            # ì „ì••: â‰¤3000V
            volt_cols = [col for col in df.columns if '_volt' in col.lower()]
            for col in volt_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and val > 3000) else val)
            
            # ì˜¨ë„: -35~80Â°C
            temp_cols = [col for col in df.columns if '_temp' in col.lower()]
            for col in temp_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and (val < -35 or val > 80)) else val)
            
            # ì „ë¥˜: -500~500A
            current_cols = [col for col in df.columns if '_current' in col.lower()]
            for col in current_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and (val < -500 or val > 500)) else val)
            
            # ì°¨ëŸ‰ ì†ë„: 0~180km/h
            speed_cols = [col for col in df.columns if 'emobility_spd' in col.lower()]
            for col in speed_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and (val < 0 or val > 180)) else val)
            
            # ì…€ ì „ì••: 0~6V
            cell_volt_cols = [col for col in df.columns if 'cell_volt_' in col.lower()]
            for col in cell_volt_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and (val < 0 or val > 6)) else val)
            
            # ëˆ„ì ê°’: â‰¤1,000,000
            cumul_cols = [col for col in df.columns if 'cumul' in col.lower()]
            for col in cumul_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and val > 1000000) else val)
            
            # ì£¼í–‰ê±°ë¦¬: 0~2,000,000km
            odo_cols = [col for col in df.columns if 'odometer' in col.lower()]
            for col in odo_cols:
                df[col] = df[col].map(lambda val: np.nan if (isinstance(val, (int, float)) and (val <= 0 or val > 2000000)) else val)
        
        elif category == 'gps':
            # ìœ„ë„: -90~90
            if 'lat' in df.columns:
                df['lat'] = df['lat'].apply(lambda x: np.nan if pd.notna(x) and (x < -90 or x > 90) else x)
            
            # ê²½ë„: -180~180
            if 'lng' in df.columns:
                df['lng'] = df['lng'].apply(lambda x: np.nan if pd.notna(x) and (x < -180 or x > 180) else x)
            
            # ì†ë„: 0~300km/h
            if 'speed' in df.columns:
                df['speed'] = df['speed'].apply(lambda x: np.nan if pd.notna(x) and (x < 0 or x > 300) else x)
            
            # ë°©í–¥: 0~360
            if 'direction' in df.columns:
                df['direction'] = df['direction'].apply(lambda x: np.nan if pd.notna(x) and (x < 0 or x > 360) else x)
            
            # ì—°ë£Œí¼ì„¼íŠ¸: 0~100
            if 'fuel_pct' in df.columns:
                df['fuel_pct'] = df['fuel_pct'].apply(lambda x: np.nan if pd.notna(x) and (x < 0 or x > 100) else x)
            
            # HDOP: 0~50
            if 'hdop' in df.columns:
                df['hdop'] = df['hdop'].apply(lambda x: np.nan if pd.notna(x) and (x < 0 or x > 50) else x)
        
        return df
    
    def convert_data_types(self, df: pd.DataFrame, category: str) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ë³€í™˜ - ìš°ë¦¬ ì½”ë“œì˜ ì¥ì """
        if df.empty:
            return df
            
        df = df.copy()
        
        # device_no: object íƒ€ì… ìœ ì§€ (ê³¼í•™ì  í‘œê¸°ë²• ë°©ì§€)
        if 'device_no' in df.columns:
            df['device_no'] = df['device_no'].astype(str)
        
        # ì‹œê°„ ì»¬ëŸ¼ë“¤: object íƒ€ì… ìœ ì§€
        time_cols = ['time', 'msg_time', 'measured_month']
        for col in time_cols:
            if col in df.columns:
                df[col] = df[col].astype(str)
        
        if category == 'bms':
            # BMS ìˆ«ì ì»¬ëŸ¼ë“¤ì„ float64ë¡œ ë³€í™˜
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if col not in ['device_no', 'time', 'msg_time', 'measured_month']:
                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')
        
        elif category == 'gps':
            # GPS ìˆ«ì ì»¬ëŸ¼ë“¤ì„ float64ë¡œ ë³€í™˜
            float_cols = ['lat', 'lng', 'speed', 'direction', 'fuel_pct', 'hdop']
            for col in float_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')
            
            # GPS ë¬¸ì ì»¬ëŸ¼ë“¤ì„ objectë¡œ ë³€í™˜
            object_cols = ['mode', 'source', 'state', 'car_type']
            for col in object_cols:
                if col in df.columns:
                    df[col] = df[col].astype('object')
        
        return df
    
    def process_file_streaming_append(self, file_path: str, category: str, output_file: str):
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì²˜ë¦¬ - append ëª¨ë“œ (í—¤ë” ì—†ì´)"""
        try:
            chunk_size = 10000  # 1ë§Œ í–‰ì”© ì²˜ë¦¬
            
            print(f"ğŸ”„ {file_path} append ì²˜ë¦¬ ì‹œì‘")
            
            for chunk_df in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
                if chunk_df.empty:
                    continue
                
                # ë°ì´í„° ì •ì œ
                chunk_df = self.clean_data(chunk_df, category)
                
                # GPS ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ ë™ì  í™•ì¥
                if category == 'gps':
                    chunk_df = self.expand_gps_list_columns(chunk_df)
                
                # ë¬¼ë¦¬ì  ë²”ìœ„ ê²€ì¦
                chunk_df = self.validate_physical_ranges(chunk_df, category)
                
                # ë°ì´í„° íƒ€ì… ë³€í™˜
                chunk_df = self.convert_data_types(chunk_df, category)
                                
                # í—¤ë” ì—†ì´ append
                chunk_df.to_csv(output_file, mode='a', header=False, index=False)
                
                print(f"âœ… {file_path} ì²­í¬ append ì™„ë£Œ ({len(chunk_df)}í–‰)")
                
                # ë©”ëª¨ë¦¬ì—ì„œ ì²­í¬ í•´ì œ
                del chunk_df
            
            print(f"âœ… {file_path} append ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {file_path} append ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def process_file_streaming(self, file_path: str, category: str, output_file: str):
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì """
        try:
            chunk_size = 10000  # 1ë§Œ í–‰ì”© ì²˜ë¦¬
            is_first_chunk = True
            
            print(f"ğŸ”„ {file_path} ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘")
            
            for chunk_df in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
                if chunk_df.empty:
                    continue
                
                # ë°ì´í„° ì •ì œ
                chunk_df = self.clean_data(chunk_df, category)
                
                # GPS ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ ë™ì  í™•ì¥
                if category == 'gps':
                    chunk_df = self.expand_gps_list_columns(chunk_df)
                
                # ë¬¼ë¦¬ì  ë²”ìœ„ ê²€ì¦
                chunk_df = self.validate_physical_ranges(chunk_df, category)
                
                # ë°ì´í„° íƒ€ì… ë³€í™˜
                chunk_df = self.convert_data_types(chunk_df, category)
                
                # ì²« ë²ˆì§¸ ì²­í¬ëŠ” í—¤ë”ì™€ í•¨ê»˜ ì €ì¥, ì´í›„ëŠ” í—¤ë” ì—†ì´ append
                chunk_df.to_csv(output_file, mode='a' if not is_first_chunk else 'w', 
                               header=is_first_chunk, index=False)
                
                is_first_chunk = False
                print(f"âœ… {file_path} ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ ({len(chunk_df)}í–‰)")
                
                # ë©”ëª¨ë¦¬ì—ì„œ ì²­í¬ í•´ì œ
                del chunk_df
            
            print(f"âœ… {file_path} ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {file_path} ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def process_file(self, file_path: str, category: str) -> pd.DataFrame:
        """íŒŒì¼ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        try:
            # íŒŒì¼ ì½ê¸°
            df = pd.read_csv(file_path, low_memory=False)
            
            # ë°ì´í„° ì •ì œ
            df = self.clean_data(df, category)
            
            # GPS ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ ë™ì  í™•ì¥ (ì •ì œ ì§í›„ ì ìš©)
            if category == 'gps':
                df = self.expand_gps_list_columns(df)
            
            # ë¬¼ë¦¬ì  ë²”ìœ„ ê²€ì¦
            df = self.validate_physical_ranges(df, category)
            
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            df = self.convert_data_types(df, category)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë³´ì •
            for dt_col in ['time', 'msg_time', 'measured_month', 'start_time']:
                if dt_col in df.columns:
                    df[dt_col] = self._fix_year_vectorized(df[dt_col])
            
            return df
            
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            return pd.DataFrame()
    
    def save_data(self, df: pd.DataFrame, category: str, output_dir: str):
        """í†µí•© ë°ì´í„° ì €ì¥"""
        if df.empty:
            return
        
        # ì €ì¥ ê²½ë¡œ ìƒì„±
        output_path = Path(output_dir) / f"{category}.csv"
        
        # í†µí•© íŒŒì¼ë¡œ ì €ì¥
        df.to_csv(output_path, mode='w', header=True, index=False)
        print(f"âœ… {category} í†µí•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")

    def process_directory(self, root_dir: str, output_dir: str, use_ray: bool = True):
        """splited_data êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°œë³„ íŒŒì¼ ì „ì²˜ë¦¬"""
        root = Path(root_dir)
        output = Path(output_dir)
        output.mkdir(parents=True, exist_ok=True)

        categories = ['bms', 'gps']

        try:
            if use_ray:
                import ray
                if not ray.is_initialized():
                    ray.init(ignore_reinit_error=True, logging_level=30)

                @ray.remote
                def _process_file_with_structure(file_path: str, category: str, root_dir: str, output_dir: str):
                    inst = BasePreprocessor()
                    df = inst.process_file(file_path, category)
                    
                    if df.empty:
                        return {"status": "empty", "path": file_path}
                    
                    # ì›ë³¸ êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ ì¶œë ¥ ê²½ë¡œ ìƒì„±
                    relative_path = Path(file_path).relative_to(Path(root_dir))
                    output_path = Path(output_dir) / relative_path
                    output_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    df.to_csv(output_path, index=False)
                    return {"status": "success", "path": str(relative_path)}

                for category in categories:
                    file_paths = [str(p) for p in root.rglob(f"**/{category}/**/*.csv")]
                    if not file_paths:
                        continue
                    
                    print(f"ğŸ”„ {category} íŒŒì¼ {len(file_paths)}ê°œ ì²˜ë¦¬ ì‹œì‘...")
                    
                    # Ray ë³‘ë ¬ ì²˜ë¦¬
                    futures = [_process_file_with_structure.remote(p, category, str(root), str(output)) for p in file_paths]
                    results = ray.get(futures)
                    
                    success_count = sum(1 for r in results if r["status"] == "success")
                    empty_count = sum(1 for r in results if r["status"] == "empty")
                    
                    print(f"âœ… {category} ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ë¹ˆíŒŒì¼ {empty_count}ê°œ")
                    
            else:
                for category in categories:
                    # file_paths = list(root.rglob(f"**/{category}/**/*.csv"))
                    file_paths = list(root.rglob("*.csv"))
                    if not file_paths:
                        continue
                    
                    for file_path in file_paths:
                        # ì›ë³¸ êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ ì¶œë ¥ ê²½ë¡œ ìƒì„±
                        relative_path = file_path.relative_to(root)
                        output_path = output / relative_path
                        
                        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
                        output_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬ í›„ ì €ì¥
                        self.process_file_streaming(str(file_path), category, str(output_path))
                        print(f"âœ… {relative_path} ì „ì²˜ë¦¬ ì™„ë£Œ")
                        
        except Exception as e:
            print(f"âŒ ë””ë ‰í„°ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    bp = BasePreprocessor()
    
    # splited_data í´ë” ìŠ¤ìº”í•´ì„œ ì •ì œ
    bp.process_directory(
        "splited_data",  # ì…ë ¥ í´ë”
        "final_data",    # ì¶œë ¥ í´ë”
        use_ray=False  # Ray ë³‘ë ¬ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
    )