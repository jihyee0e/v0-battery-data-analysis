#!/usr/bin/env python3
"""
ë°°í„°ë¦¬ ì„±ëŠ¥ ì‹œìŠ¤í…œì„ ìœ„í•œ ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ERD ì„¤ê³„ë¥¼ ìœ„í•œ ê¸°ë³¸ ë°ì´í„° íŠ¹ì„±ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import os
import json
from pathlib import Path
from datetime import datetime
from multiprocessing import Pool, cpu_count

def analyze_csv_file(file_path, show_columns=True):
    """CSV íŒŒì¼ì˜ ì „ì²´ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì„œ ê¸°ë³¸ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    print(f"\n{'='*60}")
    print(f"íŒŒì¼ ë¶„ì„: {os.path.basename(file_path)}")
    print(f"{'='*60}")
    
    try:
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        print(f"íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¶„ì„
        print("ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ë¶„ì„ ì¤‘...")
        chunk_size = 10000
        total_rows = 0
        columns_info = {}
        
        # ì²« ë²ˆì§¸ ì²­í¬ë¡œ ì»¬ëŸ¼ ì •ë³´ í™•ì¸
        first_chunk = pd.read_csv(file_path, nrows=1)
        columns = first_chunk.columns.tolist()
        print(f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(columns)}")
        
        # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥ (ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œë§Œ ìƒì„¸ ì¶œë ¥)
        if show_columns:
            print(f"\nì»¬ëŸ¼ ëª©ë¡:")
            for i, col in enumerate(columns):
                print(f"{i+1:3d}. {col}")
        else:
            print(f"(ì»¬ëŸ¼ ëª©ë¡ì€ ì²« ë²ˆì§¸ íŒŒì¼ê³¼ ë™ì¼)")
        
        for col in columns:
            columns_info[col] = {
                'dtype': str(first_chunk[col].dtype),
                'total_count': 0,
                'non_null_count': 0,
                'null_count': 0,
                'unique_values': set(),
                'min_val': None,
                'max_val': None,
                'zero_count': 0,
                'negative_count': 0
            }
        
        # ì²­í¬ë³„ë¡œ ì „ì²´ ë°ì´í„° ì²˜ë¦¬
        chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)
        for chunk_num, chunk in enumerate(chunk_iter):
            total_rows += len(chunk)
                        
            # ê° ì»¬ëŸ¼ë³„ í†µê³„ ëˆ„ì  (ê¸°ì¡´ ì»¬ëŸ¼ + ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼)
            for col in chunk.columns:
                if col in columns_info:
                    col_data = chunk[col]
                    col_info = columns_info[col]
                    
                    # ê¸°ë³¸ í†µê³„
                    col_info['total_count'] += len(col_data)
                    col_info['non_null_count'] += col_data.count()
                    col_info['null_count'] += col_data.isnull().sum()
                    
                    # ê³ ìœ ê°’ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ set ì‚¬ìš©)
                    if col_data.dtype == 'object':
                        col_info['unique_values'].update(col_data.dropna().astype(str))
                    else:
                        col_info['unique_values'].update(col_data.dropna())
                    
                    # ìˆ«ìí˜• ë°ì´í„° í†µê³„
                    if pd.api.types.is_numeric_dtype(col_data):
                        non_null_data = col_data.dropna()
                        if len(non_null_data) > 0:
                            if col_info['min_val'] is None:
                                col_info['min_val'] = non_null_data.min()
                                col_info['max_val'] = non_null_data.max()
                            else:
                                col_info['min_val'] = min(col_info['min_val'], non_null_data.min())
                                col_info['max_val'] = max(col_info['max_val'], non_null_data.max())
                            
                            col_info['zero_count'] += (non_null_data == 0).sum()
                            col_info['negative_count'] += (non_null_data < 0).sum()
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (chunk_num + 1) % 10 == 0:
                print(f"ì²˜ë¦¬ëœ ì²­í¬: {chunk_num + 1}, ëˆ„ì  í–‰ ìˆ˜: {total_rows:,}")
        
        print(f"ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}")
        
        # ìµœì¢… í†µê³„ ì •ë¦¬
        final_stats = {}
        for col, col_info in columns_info.items():
            final_stats[col] = {
                'dtype': col_info['dtype'],
                'total_count': col_info['total_count'],
                'non_null_count': col_info['non_null_count'],
                'null_count': col_info['null_count'],
                'null_percentage': (col_info['null_count'] / col_info['total_count']) * 100 if col_info['total_count'] > 0 else 0,
                'unique_count': len(col_info['unique_values']),
                'min_val': col_info['min_val'],
                'max_val': col_info['max_val'],
                'zero_count': col_info['zero_count'],
                'negative_count': col_info['negative_count']
            }
        
        return final_stats, total_rows, columns
        
    except Exception as e:
        print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        print(f"íŒŒì¼ ê²½ë¡œ: {file_path}")
        return None, 0, []

def analyze_column_stats(df, column_name):
    """íŠ¹ì • ì»¬ëŸ¼ì˜ ê¸°ë³¸ í†µê³„ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    if column_name not in df.columns:
        return None
    
    col_data = df[column_name]
    
    stats = {
        'column': column_name,
        'dtype': str(col_data.dtype),
        'total_count': len(col_data),
        'non_null_count': col_data.count(),
        'null_count': col_data.isnull().sum(),
        'null_percentage': (col_data.isnull().sum() / len(col_data)) * 100,
        'unique_count': col_data.nunique(),
        'duplicate_count': len(col_data) - col_data.nunique()
    }
    
    # ìˆ«ìí˜• ë°ì´í„°ì¸ ê²½ìš° ì¶”ê°€ í†µê³„
    if pd.api.types.is_numeric_dtype(col_data):
        stats.update({
            'min': col_data.min(),
            'max': col_data.max(),
            'zero_count': (col_data == 0).sum(),
            'negative_count': (col_data < 0).sum()
        })
    
    return stats

def analyze_gps_data(df):
    """GPS ë°ì´í„°ì˜ ì£¼ìš” ì»¬ëŸ¼ íŠ¹ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"\n{'='*40}")
    print("GPS ì»¬ëŸ¼ í™•ì¸")
    print(f"{'='*40}")
    
    # SQLì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ ì»¬ëŸ¼ë“¤
    core_columns = ['device_no', 'car_type', 'time', 'lat', 'lng', 'speed', 'fuel_pct']
    additional_columns = ['direction', 'hdop', 'state', 'mode']
    
    print("\nğŸ” í•µì‹¬ ì»¬ëŸ¼:")
    for col in core_columns:
        if col in df.columns:
            col_data = df[col]
            print(f"\n{col}:")
            print(f"  - ë°ì´í„° íƒ€ì…: {col_data.dtype}")
            print(f"  - null ê°œìˆ˜: {col_data.isnull().sum():,}")
            print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {col_data.nunique():,}")
            
            if pd.api.types.is_numeric_dtype(col_data):
                print(f"  - ë²”ìœ„: {col_data.min():.3f} ~ {col_data.max():.3f}")
                print(f"  - 0ê°’ ê°œìˆ˜: {(col_data == 0).sum():,}")
            else:
                print(f"  - ê³ ìœ ê°’: {col_data.unique()}")
    
    print("\nğŸ“Š ì¶”ê°€ ì»¬ëŸ¼ë“¤ (ì¼ë¶€ ë¶„ì„ì—ì„œ ì‚¬ìš©):")
    for col in additional_columns:
        if col in df.columns:
            col_data = df[col]
            print(f"\n{col}:")
            print(f"  - ë°ì´í„° íƒ€ì…: {col_data.dtype}")
            print(f"  - null ê°œìˆ˜: {col_data.isnull().sum():,}")
            print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {col_data.nunique():,}")
            
            if pd.api.types.is_numeric_dtype(col_data):
                print(f"  - ë²”ìœ„: {col_data.min():.3f} ~ {col_data.max():.3f}")
                print(f"  - 0ê°’ ê°œìˆ˜: {(col_data == 0).sum():,}")
            else:
                print(f"  - ê³ ìœ ê°’: {col_data.unique()}")

def analyze_battery_stats(stats, columns):
    """ë°°í„°ë¦¬ ê´€ë ¨ ë°ì´í„°ì˜ íŠ¹ì„±ì„ í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."""
    print(f"\n{'='*40}")
    print("ë°°í„°ë¦¬ ë°ì´í„° ë¶„ì„")
    print(f"{'='*40}")
    
    # í•µì‹¬ ì»¬ëŸ¼ë“¤
    core_columns = ['device_no', 'car_type', 'msg_time', 'soc', 'soh', 'pack_volt', 'pack_current', 
                   'mod_avg_temp', 'max_cell_volt', 'min_cell_volt']
    additional_columns = ['cellvolt_dispersion', 'odometer', 'ext_temp', 'trip_chrg_pw', 'trip_dischrg_pw',
                         'max_deter_cell_no', 'min_deter_cell_no']
    
    print("\nğŸ” í•µì‹¬ ì»¬ëŸ¼ë“¤ (ëª¨ë“  ë¶„ì„ì—ì„œ ì‚¬ìš©):")
    for col in core_columns:
        if col in stats:
            col_stat = stats[col]
            print(f"\n{col}:")
            print(f"  - ë°ì´í„° íƒ€ì…: {col_stat['dtype']}")
            print(f"  - null ê°œìˆ˜: {col_stat['null_count']:,}")
            print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {col_stat['unique_count']:,}")
            
            if col_stat['min_val'] is not None and col_stat['max_val'] is not None:
                print(f"  - ë²”ìœ„: {col_stat['min_val']:.3f} ~ {col_stat['max_val']:.3f}")
                print(f"  - 0ê°’ ê°œìˆ˜: {col_stat['zero_count']:,}")
    
    print("\nğŸ“Š ì¶”ê°€ ì»¬ëŸ¼ë“¤ (ì¼ë¶€ ë¶„ì„ì—ì„œ ì‚¬ìš©):")
    for col in additional_columns:
        if col in stats:
            col_stat = stats[col]
            print(f"\n{col}:")
            print(f"  - ë°ì´í„° íƒ€ì…: {col_stat['dtype']}")
            print(f"  - null ê°œìˆ˜: {col_stat['null_count']:,}")
            print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {col_stat['unique_count']:,}")
            
            if col_stat['min_val'] is not None and col_stat['max_val'] is not None:
                print(f"  - ë²”ìœ„: {col_stat['min_val']:.3f} ~ {col_stat['max_val']:.3f}")
                print(f"  - 0ê°’ ê°œìˆ˜: {col_stat['zero_count']:,}")

def analyze_gps_stats(stats, columns):
    """GPS ë°ì´í„°ì˜ ì£¼ìš” ì»¬ëŸ¼ íŠ¹ì„±ì„ í†µê³„ ê¸°ë°˜ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"\n{'='*40}")
    print("GPS ì»¬ëŸ¼ í™•ì¸")
    print(f"{'='*40}")
    
    # SQLì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ ì»¬ëŸ¼ë“¤
    core_columns = ['device_no', 'car_type', 'time', 'lat', 'lng', 'speed', 'fuel_pct']
    additional_columns = ['direction', 'hdop', 'state', 'mode']
    
    print("\nğŸ” í•µì‹¬ ì»¬ëŸ¼:")
    for col in core_columns:
        if col in stats:
            col_stat = stats[col]
            print(f"\n{col}:")
            print(f"  - ë°ì´í„° íƒ€ì…: {col_stat['dtype']}")
            print(f"  - null ê°œìˆ˜: {col_stat['null_count']:,}")
            print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {col_stat['unique_count']:,}")
            
            if col_stat['min_val'] is not None and col_stat['max_val'] is not None:
                print(f"  - ë²”ìœ„: {col_stat['min_val']:.3f} ~ {col_stat['max_val']:.3f}")
                print(f"  - 0ê°’ ê°œìˆ˜: {col_stat['zero_count']:,}")
    
    print("\nğŸ“Š ì¶”ê°€ ì»¬ëŸ¼ë“¤ (ì¼ë¶€ ë¶„ì„ì—ì„œ ì‚¬ìš©):")
    for col in additional_columns:
        if col in stats:
            col_stat = stats[col]
            print(f"\n{col}:")
            print(f"  - ë°ì´í„° íƒ€ì…: {col_stat['dtype']}")
            print(f"  - null ê°œìˆ˜: {col_stat['null_count']:,}")
            print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {col_stat['unique_count']:,}")
            
            if col_stat['min_val'] is not None and col_stat['max_val'] is not None:
                print(f"  - ë²”ìœ„: {col_stat['min_val']:.3f} ~ {col_stat['max_val']:.3f}")
                print(f"  - 0ê°’ ê°œìˆ˜: {col_stat['zero_count']:,}")
    

def save_analysis_results(all_results, output_dir="analysis_results"):
    """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ì „ì²´ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    json_file = output_path / f"data_analysis_summary_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    
    # 2. ì»¬ëŸ¼ë³„ ìƒì„¸ ì •ë³´ë¥¼ CSVë¡œ ì €ì¥
    csv_file = output_path / f"column_details_{timestamp}.csv"
    column_details = []
    
    for file_name, file_data in all_results['files'].items():
        for col_name, col_stats in file_data['columns'].items():
            row = {
                'file_name': file_name,
                'column_name': col_name,
                'data_type': col_stats.get('dtype', ''),
                'total_count': col_stats.get('total_count', 0),
                'null_count': col_stats.get('null_count', 0),
                'null_percentage': col_stats.get('null_percentage', 0),
                'unique_count': col_stats.get('unique_count', 0),
                'min_value': col_stats.get('min_val', ''),
                'max_value': col_stats.get('max_val', ''),
                'zero_count': col_stats.get('zero_count', ''),
                'negative_count': col_stats.get('negative_count', '')
            }
            column_details.append(row)
    
    df_details = pd.DataFrame(column_details)
    df_details.to_csv(csv_file, index=False, encoding='utf-8')
    
    # 3. ë°ì´í„° ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
    report_file = output_path / f"data_quality_report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸\n")
        f.write("="*50 + "\n\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for file_name, file_data in all_results['files'].items():
            f.write(f"íŒŒì¼: {file_name}\n")
            f.write(f"í¬ê¸°: {file_data['file_size']:.2f} MB\n")
            f.write(f"ì „ì²´ í–‰ ìˆ˜: {file_data['total_rows']:,}\n")
            f.write(f"ì»¬ëŸ¼ ìˆ˜: {file_data['total_columns']}\n\n")
            
            # ë¬¸ì œê°€ ìˆëŠ” ì»¬ëŸ¼ë“¤ ì‹ë³„
            problem_columns = []
            for col_name, col_stats in file_data['columns'].items():
                if col_stats.get('null_percentage', 0) > 50:
                    problem_columns.append(f"{col_name} (nullìœ¨: {col_stats['null_percentage']:.1f}%)")
                elif col_stats.get('unique_count', 0) == 1:
                    problem_columns.append(f"{col_name} (ê³ ìœ ê°’ 1ê°œ)")
            
            if problem_columns:
                f.write("âš ï¸ ì£¼ì˜ê°€ í•„ìš”í•œ ì»¬ëŸ¼ë“¤:\n")
                for col in problem_columns:
                    f.write(f"  - {col}\n")
            f.write("\n" + "-"*30 + "\n\n")
    
    print(f"\nğŸ“ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"  - JSON ìš”ì•½: {json_file}")
    print(f"  - ì»¬ëŸ¼ ìƒì„¸: {csv_file}")
    print(f"  - í’ˆì§ˆ ë¦¬í¬íŠ¸: {report_file}")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    base_path = Path("/mnt/hdd1/jihye0e/aicar-preprocessing/final_data/aicar_2308_splited_by_cartype")
    
    if not base_path.exists():
        print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        return
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤ ì°¾ê¸°
    bms_files = list(base_path.glob("bms/**/*.csv"))
    gps_files = list(base_path.glob("gps/**/*.csv"))
    
    print("ë°ì´í„° ì»¬ëŸ¼ ë¶„ì„ ì‹œì‘")
    print("="*60)
    print(f"ë¶„ì„í•  BMS íŒŒì¼: {len(bms_files)}ê°œ")
    print(f"ë¶„ì„í•  GPS íŒŒì¼: {len(gps_files)}ê°œ")
    
    if len(bms_files) == 0 and len(gps_files) == 0:
        print("âŒ ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    all_results = {
        'analysis_time': datetime.now().isoformat(),
        'files': {}
    }
    
    # 1ì°¨ ë¶„ì„: BMS ë°ì´í„° ë¶„ì„
    print("\nğŸ”‹ 1ì°¨ ë¶„ì„: BMS ë°ì´í„° ë¶„ì„")
    for i, file_path in enumerate(bms_files):
        if file_path.exists():
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œë§Œ ì»¬ëŸ¼ ëª©ë¡ ìƒì„¸ ì¶œë ¥
            show_columns = (i == 0)
            stats, total_rows, columns = analyze_csv_file(file_path, show_columns)
            if stats is not None:
                file_name = file_path.name
                
                file_result = {
                    'file_size': file_size,
                    'total_rows': total_rows,
                    'total_columns': len(columns),
                    'columns': stats
                }
                
                all_results['files'][file_name] = file_result
    
    # 1ì°¨ ë¶„ì„: GPS ë°ì´í„° ë¶„ì„  
    print("\n\nğŸ“ 1ì°¨ ë¶„ì„: GPS ë°ì´í„° ë¶„ì„")
    for i, file_path in enumerate(gps_files):
        if file_path.exists():
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œë§Œ ì»¬ëŸ¼ ëª©ë¡ ìƒì„¸ ì¶œë ¥
            show_columns = (i == 0)
            stats, total_rows, columns = analyze_csv_file(file_path, show_columns)
            if stats is not None:
                file_name = file_path.name
                
                file_result = {
                    'file_size': file_size,
                    'total_rows': total_rows,
                    'total_columns': len(columns),
                    'columns': stats
                }
                
                all_results['files'][file_name] = file_result
    
    # 2ì°¨ ë¶„ì„: ì°¨ì¢…ë³„ë¡œ 1ì°¨ ê²°ê³¼ë“¤ì„ ì§‘ê³„
    print(f"\nğŸ” 2ì°¨ ë¶„ì„: ì°¨ì¢…ë³„ í†µê³„ ì§‘ê³„")
    grouped_data = analyze_by_cartype_and_type(all_results['files'], bms_files + gps_files)
    
    # ê²°ê³¼ ì €ì¥
    save_analysis_results(all_results)

def analyze_by_cartype_and_type(files_data, file_paths):
    """2ì°¨ ë¶„ì„: ì°¨ì¢…ë³„ë¡œ 1ì°¨ ê²°ê³¼ë“¤ì„ ì§‘ê³„í•©ë‹ˆë‹¤."""
    print(f"\n{'='*60}")
    print("2ì°¨ ë¶„ì„: ì°¨ì¢…ë³„ í†µê³„ ì§‘ê³„")
    print(f"{'='*60}")
    
    # ì°¨ì¢…ë³„, íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
    grouped_data = {}
    
    # íŒŒì¼ ê²½ë¡œì™€ ë°ì´í„° ë§¤í•‘
    file_path_map = {}
    for file_path in file_paths:
        file_path_map[file_path.name] = file_path
    
    for file_name, file_data in files_data.items():
        # íŒŒì¼ëª…ì—ì„œ ì°¨ì¢…ê³¼ íƒ€ì… ì¶”ì¶œ
        if 'bms' in file_name.lower():
            data_type = 'BMS'
        elif 'gps' in file_name.lower():
            data_type = 'GPS'
        else:
            continue
            
        # ê²½ë¡œì—ì„œ ì°¨ì¢… ì¶”ì¶œ (ì˜ˆ: .../BONGO3/bms_xxx.csv)
        file_path = file_path_map.get(file_name)
        cartype = 'UNKNOWN'
        if file_path:
            for ct in ['BONGO3', 'GV60', 'PORTER2']:
                if ct in str(file_path):
                    cartype = ct
                    break
        
        # ê·¸ë£¹í™”
        key = f"{cartype}_{data_type}"
        if key not in grouped_data:
            grouped_data[key] = {
                'cartype': cartype,
                'data_type': data_type,
                'files': [],
                'total_files': 0,
                'total_rows': 0,
                'total_size_mb': 0,
                'column_stats': {}  # ì»¬ëŸ¼ë³„ í†µê³„ ì§‘ê³„ìš©
            }
        
        grouped_data[key]['files'].append(file_name)
        grouped_data[key]['total_files'] += 1
        grouped_data[key]['total_rows'] += file_data['total_rows']
        grouped_data[key]['total_size_mb'] += file_data['file_size']
        
        # ì»¬ëŸ¼ë³„ í†µê³„ ì§‘ê³„
        for col_name, col_stats in file_data['columns'].items():
            if col_name not in grouped_data[key]['column_stats']:
                grouped_data[key]['column_stats'][col_name] = {
                    'dtype': col_stats['dtype'],
                    'total_count': 0,
                    'null_count': 0,
                    'min_vals': [],
                    'max_vals': [],
                    'null_percentages': [],
                    'zero_counts': [],
                    'negative_counts': []
                }
            
            # í†µê³„ê°’ë“¤ ìˆ˜ì§‘
            col_agg = grouped_data[key]['column_stats'][col_name]
            col_agg['total_count'] += col_stats['total_count']
            col_agg['null_count'] += col_stats['null_count']
            
            # min/max ê°’ë“¤ ìˆ˜ì§‘
            if col_stats.get('min_val') is not None:
                col_agg['min_vals'].append(col_stats['min_val'])
            if col_stats.get('max_val') is not None:
                col_agg['max_vals'].append(col_stats['max_val'])
            
            # nullìœ¨, 0ê°’, ìŒìˆ˜ê°’ ìˆ˜ì§‘
            col_agg['null_percentages'].append(col_stats.get('null_percentage', 0))
            col_agg['zero_counts'].append(col_stats.get('zero_count', 0))
            col_agg['negative_counts'].append(col_stats.get('negative_count', 0))
    
    # ê° ê·¸ë£¹ë³„ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    for key, group_data in grouped_data.items():
        print(f"\nğŸš— {group_data['cartype']} - {group_data['data_type']}")
        print(f"   íŒŒì¼ ìˆ˜: {group_data['total_files']}ê°œ")
        print(f"   ì´ í–‰ ìˆ˜: {group_data['total_rows']:,}")
        print(f"   ì´ í¬ê¸°: {group_data['total_size_mb']:.2f} MB")
        
        # ëª¨ë“  ì»¬ëŸ¼ ë¶„ì„ (ì°¨ì¢…ë³„ í†µí•© í†µê³„)
        print(f"   ğŸ“Š ëª¨ë“  ì»¬ëŸ¼ í†µí•© í†µê³„:")
        for col in group_data['column_stats'].keys():
            col_agg = group_data['column_stats'][col]
            
            # í†µí•© í†µê³„ ê³„ì‚°
            overall_min = min(col_agg['min_vals']) if col_agg['min_vals'] else None
            overall_max = max(col_agg['max_vals']) if col_agg['max_vals'] else None
            avg_null_percentage = sum(col_agg['null_percentages']) / len(col_agg['null_percentages']) if col_agg['null_percentages'] else 0
            total_zero_count = sum(col_agg['zero_counts'])
            total_negative_count = sum(col_agg['negative_counts'])
            
            print(f"     {col}:")
            print(f"       ë°ì´í„° íƒ€ì…: {col_agg['dtype']}")
            print(f"       ì „ì²´ í–‰ ìˆ˜: {col_agg['total_count']:,}")
            print(f"       ì „ì²´ null ìˆ˜: {col_agg['null_count']:,}")
            print(f"       í‰ê·  nullìœ¨: {avg_null_percentage:.1f}%")
            
            if overall_min is not None and overall_max is not None:
                print(f"       ì „ì²´ ë²”ìœ„: {overall_min:.3f} ~ {overall_max:.3f}")
            if total_zero_count > 0:
                print(f"       ì „ì²´ 0ê°’ ìˆ˜: {total_zero_count:,}")
            if total_negative_count > 0:
                print(f"       ì „ì²´ ìŒìˆ˜ê°’ ìˆ˜: {total_negative_count:,}")
    
    return grouped_data

if __name__ == "__main__":
    main()